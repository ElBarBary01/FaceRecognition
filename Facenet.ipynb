{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Function to detect faces in an image\n",
    "def detect_faces(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13196 files belonging to 5746 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the LFW dataset\n",
    "lfw_data_dir = \"./lfw\"\n",
    "lfw_data = tf.keras.preprocessing.image_dataset_from_directory(lfw_data_dir, labels=\"inferred\", label_mode=\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omarm\\AppData\\Local\\Temp\\ipykernel_24832\\3069859975.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array(X)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the images and labels\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for images, labels in lfw_data:\n",
    "    for image, label in zip(images, labels):\n",
    "        faces = detect_faces(image.numpy().astype(np.uint8))\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = image[y:y+h, x:x+w]\n",
    "            X.append(face_img)\n",
    "            Y.append(label.numpy())\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Facenet\n",
    "def preprocess(x):\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Create the Facenet model using MobileNetV2 as the base model\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = MobileNetV2(input_shape=input_shape, include_top=False, pooling='avg')\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Lambda(preprocess)(inputs)\n",
    "x = base_model(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "facenet = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "facenet.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                             zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "# Train the model\n",
    "facenet.fit(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test, y_test), epochs=10)\n",
    "\n",
    "# Save the model\n",
    "facenet.save('facenet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recognize faces in an image\n",
    "def recognize_faces(img, model):\n",
    "    faces = detect_faces(img)\n",
    "    face_imgs = []\n",
    "    face_rects = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        face_imgs.append(face_img)\n",
    "        face_rects.append((x, y, w, h))\n",
    "\n",
    "    face_imgs = np.array(face_imgs)\n",
    "    preds = model.predict(face_imgs)\n",
    "\n",
    "    recognized_faces = []\n",
    "    for pred, rect in zip(preds, face_rects):\n",
    "        label = np.argmax(pred)\n",
    "        confidence = np.max(pred)\n",
    "        recognized_faces.append((label, confidence, rect))\n",
    "\n",
    "    return recognized_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad marshal data (unknown type code)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\omarm\\OneDrive\\Desktop\\Face recognition\\Facenet.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omarm/OneDrive/Desktop/Face%20recognition/Facenet.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the trained Facenet model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/omarm/OneDrive/Desktop/Face%20recognition/Facenet.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m facenet \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39m./facenet_model.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omarm/OneDrive/Desktop/Face%20recognition/Facenet.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Read an input image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/omarm/OneDrive/Desktop/Face%20recognition/Facenet.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m input_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39m./people.png\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39mcustom_objects, \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\generic_utils.py:102\u001b[0m, in \u001b[0;36mfunc_load\u001b[1;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeEncodeError\u001b[39;00m, binascii\u001b[39m.\u001b[39mError):\n\u001b[0;32m    101\u001b[0m     raw_code \u001b[39m=\u001b[39m code\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mraw_unicode_escape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m code \u001b[39m=\u001b[39m marshal\u001b[39m.\u001b[39;49mloads(raw_code)\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m globs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     globs \u001b[39m=\u001b[39m \u001b[39mglobals\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: bad marshal data (unknown type code)"
     ]
    }
   ],
   "source": [
    "# Load the trained Facenet model\n",
    "facenet = tf.keras.models.load_model('./facenet_model.h5')\n",
    "\n",
    "# Read an input image\n",
    "input_image = cv2.imread('./people.png')\n",
    "\n",
    "# Recognize faces in the input image\n",
    "recognized_faces = recognize_faces(input_image, facenet)\n",
    "\n",
    "# Draw rectangles and labels around recognized faces\n",
    "for (label, confidence, (x, y, w, h)) in recognized_faces:\n",
    "    cv2.rectangle(input_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    cv2.putText(input_image, f\"{label}: {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "# Save the output image\n",
    "cv2.imwrite('./', input_image)\n",
    "\n",
    "# Show the output image\n",
    "cv2.imshow(input_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
