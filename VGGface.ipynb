{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras.utils as image\n",
    "\n",
    "from PIL import Image\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = input('\\n enter your name ==>  ')\n",
    "os.makedirs(\"Dataset/\"+folder_name, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "cam = cv2.VideoCapture(0)  # 0 represents the default camera device\n",
    "count=0\n",
    "while(True):\n",
    "    ret, frame = cam.read()\n",
    "    #img = cv2.flip(img, -1) # flip video image vertically\n",
    "    # Waiting for 250 miliseconds between every 2 captures\n",
    "    k = cv2.waitKey(250) & 0xff\n",
    "    count += 1\n",
    "    # Save the captured image into the datasets folder\n",
    "    image_path = os.path.join(\"dataset/\"+folder_name, f\"{count}.jpg\")\n",
    "    cv2.imwrite(image_path, frame)\n",
    "    # Press 'ESC' for exiting video \n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 100:\n",
    "         break\n",
    "    # Take 100 face samples for training and change the label in the image path to 'test'\n",
    "    \n",
    "print(\"\\n Exiting Program and cleanup stuff\")\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# for face detection\n",
    "face_cascade =  cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# resolution of the webcam\n",
    "screen_width = 1280       # try 640 if code fails\n",
    "screen_height = 720\n",
    "\n",
    "# default webcam\n",
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # capture frame-by-frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # try to detect faces in the webcam\n",
    "    faces = face_cascade.detectMultiScale(rgb, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # for each faces found\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        color = (0, 255, 255) # in BGR\n",
    "        stroke = 5\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, stroke)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        break             # of the loop\n",
    "\n",
    "# cleanup\n",
    "stream.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGFace(model='senet50')\n",
    "\n",
    "# load the image\n",
    "img = image.load_img(\n",
    "    './Matthias_Sammer.png',\n",
    "    target_size=(224, 224))\n",
    "\n",
    "# prepare the image\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = utils.preprocess_input(x, version=1)\n",
    "\n",
    "# perform prediction\n",
    "preds = model.predict(x)\n",
    "print('Predicted:', utils.decode_predictions(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'dataset'\n",
    "\n",
    "# dimension of images\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# for detecting faces\n",
    "facecascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# set the directory containing the images\n",
    "images_dir = os.path.join(\".\", folder_name)\n",
    "\n",
    "current_id = 0\n",
    "label_ids = {}\n",
    "\n",
    "# iterates through all the files in each subdirectories\n",
    "for root, _, files in os.walk(images_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\"png\") or file.endswith(\"jpg\") or file.endswith(\"jpeg\") :\n",
    "            # path of the image\n",
    "            path = os.path.join(root, file)\n",
    "\n",
    "            # get the label name (name of the person)\n",
    "            label = os.path.basename(root).replace(\" \", \".\").lower()\n",
    "\n",
    "        # add the label (key) and its number (value)\n",
    "        if not label in label_ids:\n",
    "            label_ids[label] = current_id\n",
    "            current_id += 1\n",
    "\n",
    "        # load the image\n",
    "        imgtest = cv2.imread(path)\n",
    "        gray = cv2.cvtColor(imgtest, cv2.COLOR_BGR2GRAY)\n",
    "        image_array = np.array(imgtest)\n",
    "\n",
    "        # get the faces detected in the image\n",
    "        faces =facecascade.detectMultiScale(gray,scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        # if not exactly 1 face is detected, skip this photo\n",
    "        if len(faces) != 1:\n",
    "            print(f'---Photo skipped---\\n')\n",
    "            # remove the original image\n",
    "            os.remove(path)\n",
    "            continue\n",
    "\n",
    "        # save the detected face(s) and associate\n",
    "        # them with the label\n",
    "        for (x_, y_, w, h) in faces:\n",
    "\n",
    "                # draw the face detected\n",
    "            face_detect = cv2.rectangle(imgtest,\n",
    "                    (x_, y_),\n",
    "                    (x_+w, y_+h),\n",
    "                    (255, 0, 255), 2)\n",
    "            plt.imshow(face_detect,cmap =\"gray\")\n",
    "            plt.show()\n",
    "\n",
    "                # resize the detected face to 224x224\n",
    "            size = (image_width, image_height)\n",
    "\n",
    "                # detected face region\n",
    "            roi = image_array[y_: y_ + h, x_: x_ + w]\n",
    "\n",
    "            # resize the detected head to target size\n",
    "            resized_image = cv2.resize(roi, size)\n",
    "            image_array = np.array(resized_image, \"uint8\")\n",
    "\n",
    "            # remove the original image\n",
    "            os.remove(path)\n",
    "\n",
    "                # replace the image with only the face\n",
    "            im = Image.fromarray(image_array)\n",
    "            im.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "'./Dataset',\n",
    "target_size=(224,224),\n",
    "color_mode='rgb',\n",
    "batch_size=32,\n",
    "class_mode='categorical',\n",
    "shuffle=True)\n",
    "\n",
    "print(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices.values()\n",
    "# dict_values([0, 1, 2])\n",
    "NO_CLASSES = len(train_generator.class_indices.values())\n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "base_model = VGGFace(include_top=True,\n",
    "    model='vgg16',\n",
    "    input_shape=(224, 224, 3))\n",
    "base_model.summary()\n",
    "\n",
    "print(len(base_model.layers))\n",
    "# 26 layers in the original VGG-Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGGFace(include_top=False,\n",
    "model='vgg16',\n",
    "input_shape=(224, 224, 3))\n",
    "base_model.summary()\n",
    "print(len(base_model.layers))\n",
    "# 19 layers after excluding the last few layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# final layer with softmax activation\n",
    "preds = Dense(NO_CLASSES, activation='sigmoid')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model with the base model's original input and the \n",
    "# new model's output\n",
    "model = Model(inputs = base_model.input, outputs = preds)\n",
    "\n",
    "# don't train the first 19 layers - 0..18\n",
    "for layer in model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# train the rest of the layers - 19 onwards\n",
    "for layer in model.layers[19:]:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "model.fit(train_generator,\n",
    "  batch_size = 4,\n",
    "  verbose = 1,\n",
    "  epochs = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a HDF5 file\n",
    "model.save(\n",
    "    'transfer_learning_trained' +\n",
    "    '_face_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# deletes the existing model\n",
    "del model\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model(\n",
    "    'transfer_learning_trained' +\n",
    "    '_face_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class_dictionary = train_generator.class_indices\n",
    "class_dictionary = {\n",
    "    value:key for key, value in class_dictionary.items()\n",
    "}\n",
    "print(class_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the class dictionary to pickle\n",
    "face_label_filename = 'face-labels.pickle'\n",
    "with open(face_label_filename, 'wb') as f: pickle.dump(class_dictionary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.utils as image\n",
    "from keras_vggface import utils\n",
    "\n",
    "# dimension of images\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# load the training labels\n",
    "face_label_filename = 'face-labels.pickle'\n",
    "with open(face_label_filename, \"rb\") as f: class_dictionary = pickle.load(f)\n",
    "\n",
    "class_list = [value for _, value in class_dictionary.items()]\n",
    "print(class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for detecting faces\n",
    "facecascade =  cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "for i in range(1,6): \n",
    "    test_image_filename = f'./facetest/{i}.jpg'\n",
    "    # load the image\n",
    "    imgtest = cv2.imread(test_image_filename, cv2.IMREAD_COLOR)\n",
    "    image_array = np.array(imgtest, \"uint8\")\n",
    "\n",
    "    # get the faces detected in the image\n",
    "    faces = facecascade.detectMultiScale(imgtest, \n",
    "        scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # if not exactly 1 face is detected, skip this photo\n",
    "    if len(faces) != 1: \n",
    "        print(f'---We need exactly 1 face; photo skipped---')\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    for (x_, y_, w, h) in faces:\n",
    "        # draw the face detected\n",
    "        face_detect = cv2.rectangle(\n",
    "            imgtest, (x_, y_), (x_+w, y_+h), (255, 0, 255), 2)\n",
    "        plt.imshow(face_detect)\n",
    "        plt.show()\n",
    "\n",
    "        # resize the detected face to 224x224\n",
    "        size = (image_width, image_height)\n",
    "        roi = image_array[y_: y_ + h, x_: x_ + w]\n",
    "        resized_image = cv2.resize(roi, size)\n",
    "\n",
    "        # prepare the image for prediction\n",
    "        x = image.img_to_array(resized_image)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = utils.preprocess_input(x, version=1)\n",
    "\n",
    "        # making prediction\n",
    "        predicted_prob = model.predict(x)\n",
    "        print(predicted_prob)\n",
    "        print(predicted_prob[0].argmax())\n",
    "        print(\"Predicted face: \" + class_list[predicted_prob[0].argmax()])\n",
    "        print(\"============================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# for face detection\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# resolution of the webcam\n",
    "screen_width = 1280       # try 640 if code fails\n",
    "screen_height = 720\n",
    "\n",
    "# size of the image to predict\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# load the trained model\n",
    "model = load_model('transfer_learning_trained_face_cnn_model.h5')\n",
    "\n",
    "# the labels for the trained model\n",
    "with open(\"face-labels.pickle\", 'rb') as f:\n",
    "    og_labels = pickle.load(f)\n",
    "    labels = {key:value for key,value in og_labels.items()}\n",
    "    print(labels)\n",
    "\n",
    "# default webcam\n",
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # try to detect faces in the webcam\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        rgb, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # for each faces found\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_rgb = rgb[y:y+h, x:x+w]\n",
    "        # Draw a rectangle around the face\n",
    "        color = (255, 0, 0) # in BGR\n",
    "        stroke = 2\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, stroke)\n",
    "\n",
    "        # resize the image\n",
    "        size = (image_width, image_height)\n",
    "        resized_image = cv2.resize(roi_rgb, size)\n",
    "        image_array = np.array(resized_image, \"uint8\")\n",
    "        img = image_array.reshape(1,image_width,image_height,3) \n",
    "        img = img.astype('float32')\n",
    "        img /= 255\n",
    "\n",
    "        # predict the image\n",
    "        predicted_prob = model.predict(img)\n",
    "\n",
    "        # Display the label\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        name = labels[predicted_prob[0].argmax()]\n",
    "        color = (255, 0, 255)\n",
    "        stroke = 2\n",
    "        cv2.putText(frame, f'({name})', (x,y-8),\n",
    "            font, 1, color, stroke, cv2.LINE_AA)\n",
    "\n",
    "        # Show the frame\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    k = cv2.waitKey(250) & 0xff\n",
    "    # Press 'ESC' for exiting video \n",
    "    if k == 27:\n",
    "     break      \n",
    "\n",
    "# Cleanup\n",
    "stream.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
